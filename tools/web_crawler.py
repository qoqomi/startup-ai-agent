"""
Web Crawler Tool - BeautifulSoup ê¸°ë°˜ ì›¹ í¬ë¡¤ë§

Tavily API rate limitì„ í”¼í•˜ê¸° ìœ„í•œ ëŒ€ì²´ ê²€ìƒ‰ ë„êµ¬
"""

from __future__ import annotations

import re
import time
from typing import Dict, List, Optional
from urllib.parse import quote_plus

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    requests = None  # type: ignore[assignment]
    BeautifulSoup = None  # type: ignore[assignment]


class WebCrawler:
    """BeautifulSoup ê¸°ë°˜ ì›¹ í¬ë¡¤ëŸ¬"""

    def __init__(self, delay: float = 1.0):
        """
        Args:
            delay: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        """
        self.delay = delay
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

    def naver_news_crawl(self, news_url: str) -> Optional[str]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        if not requests or not BeautifulSoup:
            print("âš ï¸ requests ë˜ëŠ” BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        try:
            response = requests.get(news_url, headers=self.headers, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")

                # ì œëª© ì¶”ì¶œ
                title_elem = soup.find("h2", id="title_area")
                if not title_elem:
                    title_elem = soup.find("h3", class_="tts_head")
                title = title_elem.get_text() if title_elem else ""

                # ë³¸ë¬¸ ì¶”ì¶œ
                content_elem = soup.find("div", id="contents")
                if not content_elem:
                    content_elem = soup.find("div", id="articleBodyContents")
                if not content_elem:
                    content_elem = soup.find("div", class_="article_body")
                content = content_elem.get_text() if content_elem else ""

                # ì •ë¦¬
                cleaned_title = re.sub(r"\n{2,}", "\n", title.strip())
                cleaned_content = re.sub(r"\n{2,}", "\n", content.strip())

                return f"{cleaned_title}\n{cleaned_content}"
            else:
                print(f"âš ï¸ HTTP ìš”ì²­ ì‹¤íŒ¨. ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                return None

        except Exception as e:
            print(f"âš ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    def naver_search(self, query: str, max_results: int = 5) -> List[Dict[str, str]]:
        """ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§"""
        if not requests or not BeautifulSoup:
            print("âš ï¸ requests ë˜ëŠ” BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        try:
            # ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰
            encoded_query = quote_plus(query)
            search_url = f"https://search.naver.com/search.naver?query={encoded_query}"

            response = requests.get(search_url, headers=self.headers, timeout=10)

            if response.status_code != 200:
                print(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []

            soup = BeautifulSoup(response.text, "html.parser")
            results = []

            # 1. ë‰´ìŠ¤ ê²°ê³¼ ì¶”ì¶œ
            news_items = soup.select(".news_area, .news_wrap")
            for item in news_items[:max_results]:
                title_elem = item.select_one(".news_tit, a.news_tit")
                desc_elem = item.select_one(".news_dsc, .dsc_txt_wrap")

                if title_elem:
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get("href", "")
                    description = desc_elem.get_text(strip=True) if desc_elem else title

                    results.append(
                        {
                            "title": title,
                            "url": url,
                            "content": description,
                            "snippet": description,
                        }
                    )

            # 2. ë¸”ë¡œê·¸/ì¹´í˜ ê²°ê³¼ ì¶”ê°€
            if len(results) < max_results:
                blog_items = soup.select(".total_wrap, .api_subject_bx")
                for item in blog_items[: max_results - len(results)]:
                    title_elem = item.select_one(".total_tit, .api_txt_lines")
                    desc_elem = item.select_one(".total_txt, .api_txt_lines")

                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        url_elem = item.select_one("a")
                        url = url_elem.get("href", "") if url_elem else ""
                        description = (
                            desc_elem.get_text(strip=True) if desc_elem else title
                        )

                        results.append(
                            {
                                "title": title,
                                "url": url,
                                "content": description,
                                "snippet": description,
                            }
                        )

            # 3. í†µí•©ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¼ë°˜ ì›¹ ê²°ê³¼)
            if len(results) < max_results:
                web_items = soup.select(".total_area, .bx")
                for item in web_items[: max_results - len(results)]:
                    title_elem = item.select_one(".link_tit, .tit")
                    desc_elem = item.select_one(".total_dsc, .dsc")

                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        url = title_elem.get("href", "")
                        description = (
                            desc_elem.get_text(strip=True) if desc_elem else title
                        )

                        results.append(
                            {
                                "title": title,
                                "url": url,
                                "content": description,
                                "snippet": description,
                            }
                        )

            time.sleep(self.delay)  # Rate limiting
            return results[:max_results]

        except Exception as e:
            print(f"âš ï¸ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def google_search(self, query: str, max_results: int = 5) -> List[Dict[str, str]]:
        """êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ (ê°„ë‹¨í•œ ë²„ì „)"""
        if not requests or not BeautifulSoup:
            print("âš ï¸ requests ë˜ëŠ” BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        try:
            encoded_query = quote_plus(query)
            search_url = f"https://www.google.com/search?q={encoded_query}&hl=ko"

            response = requests.get(search_url, headers=self.headers, timeout=10)

            if response.status_code != 200:
                print(f"âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []

            soup = BeautifulSoup(response.text, "html.parser")
            results = []

            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
            search_items = soup.select(".g")[:max_results]
            for item in search_items:
                title_elem = item.select_one("h3")
                link_elem = item.select_one("a")
                snippet_elem = item.select_one(".VwiC3b")

                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    url = link_elem.get("href", "")
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    results.append(
                        {
                            "title": title,
                            "url": url,
                            "content": snippet,
                            "snippet": snippet,
                        }
                    )

            time.sleep(self.delay)  # Rate limiting
            return results[:max_results]

        except Exception as e:
            print(f"âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def hybrid_search(
        self,
        query: str,
        max_results: int = 5,
        use_naver: bool = True,
        use_google: bool = False,
    ) -> List[Dict[str, str]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë„¤ì´ë²„ + êµ¬ê¸€)"""
        results = []

        if use_naver:
            naver_results = self.naver_search(query, max_results // 2 + 1)
            results.extend(naver_results)

        if use_google and len(results) < max_results:
            remaining = max_results - len(results)
            google_results = self.google_search(query, remaining)
            results.extend(google_results)

        return results[:max_results]


def search_with_crawler(query: str, max_results: int = 5) -> List[Dict[str, str]]:
    """í¬ë¡¤ëŸ¬ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ (ì™¸ë¶€ì—ì„œ ì‚¬ìš©)"""
    crawler = WebCrawler(delay=1.0)
    return crawler.hybrid_search(query, max_results, use_naver=True, use_google=False)


# ë°ëª¨
if __name__ == "__main__":
    crawler = WebCrawler()

    print("=" * 80)
    print("ğŸ” ë„¤ì´ë²„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    results = crawler.naver_search("ìš°ì£¼ì‚°ì—… ìŠ¤íƒ€íŠ¸ì—…", max_results=3)
    for idx, result in enumerate(results, 1):
        print(f"\n[{idx}] {result['title']}")
        print(f"    URL: {result['url']}")
        print(f"    ë‚´ìš©: {result['content'][:100]}...")

    print("\n" + "=" * 80)
